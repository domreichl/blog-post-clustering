{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blog Post Clustering â€“ Part 3: Autoencoder\n",
    "\n",
    "After comparing different models and model combinations for clustering my blog posts (see <a href=\"https://github.com/domreichl/blog-post-clustering/blob/master/bp_clustering_part2.ipynb\">Part 2</a>), I now want to try a more complex model, in particular, an autoencoder with three layers.\n",
    "\n",
    "Autoencoders are neural networks used for unsupervised learning and a powerful tool for dealing with the curse of dimensionality. More precisely, an autoencoder consists of two parts: an *encoder* with multiple layers to reduce dimensionality and (2) a *decoder* with multiple layers to reconstruct the input from the dimensionally-reduced data. By reconstructing its inputs, the network detects the most important features in the data as it learns the identity function under the constraint of reduced dimensionality (or added noise). Since clustering is a form of dimensionality reduction, autoencoders should be useful for categorizing my blog posts into four broad topics.\n",
    "\n",
    "Overview:\n",
    "1. Modules & Data\n",
    "2. Vectorization\n",
    "3. Autoencoder\n",
    "4. Evaluation\n",
    "5. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Modules & Data\n",
    "\n",
    "Again, I import all modules, load my blog posts, filter them, and convert the html code into text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import tensorflow as tf\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import NMF, TruncatedSVD\n",
    "from sklearn.metrics import silhouette_score, calinski_harabaz_score\n",
    "\n",
    "data = pd.read_csv('wp_posts.csv', sep=';')\n",
    "data = data[(data['post_type'] == 'post') & (data['post_status'] == 'publish')]\n",
    "data = data[['post_content']].reset_index(drop=True)\n",
    "\n",
    "for i in data.index:\n",
    "    soup = BeautifulSoup(data['post_content'].loc[i], 'html.parser')\n",
    "    data['post_content'].loc[i] = soup.get_text().lower()\n",
    "\n",
    "data = data['post_content']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Vectorization\n",
    "\n",
    "As before, I use tf-idf vectorization, but now with a lower bound on document frequency (min_df), which sets a cut-off threshold to ignore the rarest words. This allows the neural network to be trained much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words='english', min_df = 0.1)\n",
    "tfidf_matrix = vectorizer.fit_transform(data)\n",
    "words = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Autoencoder\n",
    "\n",
    "I use TensorFlow to build an autoencoder. First, I set two hyperparameters (learning rate and number of epochs) as well as the network parameters (numbers of nodes for three layers). Then, after defining the graph input (X) and all weights and biases, initialized with normally-distributed random numbers, I build an encoder and a decoder, both with sigmoid activation functions for each layer, construct the model, and define the functions for loss and optimization: minimize squared error. Finally, I initialize the variables and launch the graph before I run the session and training cycles. In the last two lines, I store the results and end the training session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 000 cost = 0.098398253\n",
      "Epoch: 010 cost = 0.016734146\n",
      "Epoch: 020 cost = 0.007022711\n",
      "Epoch: 030 cost = 0.002175688\n",
      "Epoch: 040 cost = 0.002173345\n",
      "Epoch: 050 cost = 0.002157870\n",
      "Epoch: 060 cost = 0.002237542\n",
      "Epoch: 070 cost = 0.002191694\n",
      "Epoch: 080 cost = 0.002183638\n",
      "Epoch: 090 cost = 0.002218124\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "training_epochs = 100\n",
    "\n",
    "n_input = tfidf_matrix.shape[1]\n",
    "n_hidden_1 = tfidf_matrix.shape[1] // 2\n",
    "n_hidden_2 = 4\n",
    "\n",
    "X = tf.placeholder(\"float\", [None, n_input])\n",
    "\n",
    "weights = {\n",
    "    'encoder_h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
    "    'encoder_h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "    'decoder_h1': tf.Variable(tf.random_normal([n_hidden_2, n_hidden_1])),\n",
    "    'decoder_h2': tf.Variable(tf.random_normal([n_hidden_1, n_input])),\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'encoder_b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'encoder_b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'decoder_b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'decoder_b2': tf.Variable(tf.random_normal([n_input])),\n",
    "}\n",
    "\n",
    "def encoder(x):\n",
    "    layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights['encoder_h1']), biases['encoder_b1']))\n",
    "    layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, weights['encoder_h2']), biases['encoder_b2']))\n",
    "    return layer_2\n",
    "\n",
    "def decoder(x):\n",
    "    layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights['decoder_h1']),biases['decoder_b1']))\n",
    "    layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, weights['decoder_h2']), biases['decoder_b2']))\n",
    "    return layer_2\n",
    "\n",
    "enc = encoder(X)\n",
    "dec = decoder(enc)\n",
    "\n",
    "cost = tf.reduce_mean(tf.pow(X - dec, 2))\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.InteractiveSession() # interactive for jupyter notebook\n",
    "sess.run(init)\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    for i in range(len(data)): # one batch per blog post\n",
    "        _, c = sess.run([optimizer, cost], feed_dict={X: tfidf_matrix[i].toarray()})\n",
    "    if epoch % 10 == 0: # display every tenth epoch\n",
    "        print(\"Epoch:\", '%03d' % epoch, \"cost =\", \"{:.9f}\".format(c))\n",
    "\n",
    "autoenc_results = dec.eval(feed_dict={X: tfidf_matrix.toarray()})      \n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss decreased quite well (much better than with all the other variations and parameters I tried), but 100 epochs wouldn't have been necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Evaluation\n",
    "\n",
    "In this final step, I use some models we already know from Part 2 to build an evaluation table for their scores on three metrics, now including a column for autoencoder-based k-means (km_autoenc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Silhouette</th>\n",
       "      <th>WCSS</th>\n",
       "      <th>Calinski-Harabasz</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>km</td>\n",
       "      <td>0.030429</td>\n",
       "      <td>262.18</td>\n",
       "      <td>7.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>km_nmf</td>\n",
       "      <td>0.032847</td>\n",
       "      <td>2.59</td>\n",
       "      <td>8.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>km_lsa</td>\n",
       "      <td>0.033674</td>\n",
       "      <td>10.73</td>\n",
       "      <td>8.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>km_autoenc</td>\n",
       "      <td>0.017647</td>\n",
       "      <td>3.13</td>\n",
       "      <td>6.07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Model  Silhouette    WCSS  Calinski-Harabasz\n",
       "0          km    0.030429  262.18               7.69\n",
       "1      km_nmf    0.032847    2.59               8.47\n",
       "2      km_lsa    0.033674   10.73               8.48\n",
       "3  km_autoenc    0.017647    3.13               6.07"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = 4 # number of clusters\n",
    "\n",
    "# NMF & LSA\n",
    "nmf = NMF(k)\n",
    "nmf_matrix = nmf.fit_transform(tfidf_matrix)\n",
    "lsa = TruncatedSVD(k)\n",
    "lsa_matrix = lsa.fit_transform(tfidf_matrix)\n",
    "\n",
    "# k-means variations\n",
    "km = KMeans(k).fit(tfidf_matrix)\n",
    "km_nmf = KMeans(k).fit(nmf_matrix)\n",
    "km_lsa = KMeans(k).fit(lsa_matrix)\n",
    "km_autoenc = KMeans(k).fit(autoenc_results)\n",
    "\n",
    "# evaluation table\n",
    "evaluation = pd.DataFrame({'Model': ['km', 'km_nmf', 'km_lsa', 'km_autoenc']})\n",
    "sc, wcss, chi = [], [], []\n",
    "\n",
    "# calculate scores\n",
    "for model in (km, km_nmf, km_lsa, km_autoenc):\n",
    "    sc.append(silhouette_score(tfidf_matrix.toarray(), model.labels_))\n",
    "    wcss.append(round(model.inertia_, 2))\n",
    "    chi.append(round(calinski_harabaz_score(tfidf_matrix.toarray(), model.labels_), 2))\n",
    "\n",
    "# fill in and display evaluation table\n",
    "evaluation['Silhouette'] = sc\n",
    "evaluation['WCSS'] = wcss\n",
    "evaluation['Calinski-Harabasz'] = chi\n",
    "evaluation.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, the scores for the Autoencoder-KMeans combo aren't particularly good. It has the lowest Silhouette coefficient and the lowest Calinski-Harabasz index, and that after experimenting with tons of different parameters for the neural network. In conclusion, it seems that sticking to NMF-based KMeans or even just NMF alone is the best choice for this data set."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
